{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acoustic-mattress",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning on Stock Trading (CS230 Milestone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-senegal",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "Author: Bicheng Wang, Xinyi Zhang   \n",
    "Email: bichengw@stanford.edu, xyzh@stanford.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-organization",
   "metadata": {},
   "source": [
    "## Introdution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-tablet",
   "metadata": {},
   "source": [
    "Profitable automated trading strategy plays a critical role to investment companies and hedge funds. Given that the stock market is dynamic and complex, it is challenging to design such a strategy. The project proposes to use a deep reinforcement learning framework to learn a profitable stock trading mechanism, with the goal to optimize the cumulative return and Alpha.\n",
    "It would select S\\&P 500 Index along with its top 20 market capitalization stocks as our trading stock pool.\n",
    "The input to our algorithm is the market price for these stocks, remaining balance, current portfolio and technical indicator statistics. The model agent output is a series of trading actions among stocks. \n",
    "The available trading action options are: sell, buy and hold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-happening",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-forty",
   "metadata": {},
   "source": [
    "We choose 20 stocks with top performance in the S\\\\&500 index (exclude companies IPO after 2000) from 2000 to 2020 as our dataset. The original data is fetched from Yahoo Finance API. Each dataset row is comprised of date, open price, high price, low price, close price, volumn, ticker symbol, # day in a week. The dataset has 105680 rows in total. We split the dataset into training and test on a 90/10 basis. The training set contains data ranging from 2000 to 2018, while the test set contains data ranging from 2019 to 2020. The first 5 rows of the dataset are presented as follows:\n",
    "![](img/original_dataset_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-brush",
   "metadata": {},
   "source": [
    "## Strategy Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-average",
   "metadata": {},
   "source": [
    "### Action and Environment Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-allen",
   "metadata": {},
   "source": [
    "#### State Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-tension",
   "metadata": {},
   "source": [
    "A 337-dimensional vector consists of 17 parts of information to represent the state space of multiple stocks trading environment: \n",
    "***[b, p, s, macd, boll_ub, boll_lb, rsi_10, rsi_20, cci_10, cci_20, dx_30, close_20_sma, close_60_sma, close_120_sma, close_20_ema, close_60_ema, close_120_ema]***. \n",
    "Each component is defined as follows:\n",
    "- **b**: available balance \n",
    "- **p**: close price of each stock\n",
    "- **s**: shares owned of each stock\n",
    "- **macd**: Moving Average Convergence Divergence of each stock\n",
    "- **boll_ub**: Upper Bollinger Bands of each stock\n",
    "- **boll_lb**: Lower Bollinger Bands of each stock\n",
    "- **rsi_x**: Relative Strength Index of each stock, calculated using close price\n",
    "- **cci_x**: Commodity Channel Index of each stock, calculated using high, low and close price\n",
    "- **dx**: Directional Movement Index of each stock\n",
    "- **close_x_sma**: Simple Moving Average of each stock\n",
    "- **close_x_ema**: Exponential Moving Average of each stock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-original",
   "metadata": {},
   "source": [
    "#### Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-italic",
   "metadata": {},
   "source": [
    "For a single stock, the action space is defined as *{-k. ..., -1, 0, 1, ..., k}*, where *k* and *-k* represents the number of shares we can buy and sell. A predefined parameter is set as the maximum amount of shares for each buying/selling action. Therefore, a 21-dimensional vector will be used to represent the action space. It will be normalized to [-1, 1] since the RL algorithms A2C and PPO define the policy directly on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-stanford",
   "metadata": {},
   "source": [
    "#### Reward Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-finger",
   "metadata": {},
   "source": [
    "The reward function will be defined as the change of portfolio value when action ***a*** is taken at state ***s*** and arriving at a new state ***s'***. The goal is to design a trading strategy to maximize the change of the portfolio value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-stage",
   "metadata": {},
   "source": [
    "### Training Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-campbell",
   "metadata": {},
   "source": [
    "In the model training part, we already investigated the Actor-Critic approach and applied 2 deep reinforcement learning models--PPO and A2C. We also consider to apply the Critic based like DQN or Actor bacsed approaches like Policy Graident method later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-boards",
   "metadata": {},
   "source": [
    "#### Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-terrorist",
   "metadata": {},
   "source": [
    "**Proximal Policy Optimization** (PPO) [[1]](#1) is introduced to control the policy gradient update and ensure that the new policy will not be too different from the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-success",
   "metadata": {},
   "source": [
    "According to the **Proximal Policy Optimization Algorithms**(PPO) [[1]](#1), the algorithm determines the maximum step size and find the local maximum of the policy within the region like policy gradient method to maximize the gradient. Compared to the TRPO, it directly introduces the KL divergence item as a policy learning penalty to ensure policy learning progress. The **PPO** as the off-policy strategy uses importance sampling in the historical trading data, which would provide more advantage considering the project historical data not being enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-guitar",
   "metadata": {},
   "source": [
    "#### Advantage Actor-Critic (A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-underwear",
   "metadata": {},
   "source": [
    "In above **PPO** [[1]](#1), it would directly optimize the policy. However, it is also important to leverage the value methods—evaluated the expected return, as historical trading data already provide them well—to improve the reinforcement learning. **Advantage Actor-Critic (A2C)** [[2]](#2) method would be the another valuable option to investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-senior",
   "metadata": {},
   "source": [
    "**Advantage Actor-Critic (A2C)** [[2]](#2) is a typical actor-critic algorithm. **A2C** uses copies of the same agent working in parallel to update gradients with different data samples. Each agent works independently to interact with the same environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-affair",
   "metadata": {},
   "source": [
    "## Trading Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-handle",
   "metadata": {},
   "source": [
    " [RL Training Notebook](https://github.com/BichengWang/RL_stock_trading/blob/master/rl_portfolio_trading.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-delaware",
   "metadata": {},
   "source": [
    "[Source Code Repo](https://github.com/BichengWang/RL_stock_trading) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-updating",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-module",
   "metadata": {},
   "source": [
    "| Method  | Training Period | Evaluation Period |\n",
    "| ------------- | ------------- | ------------- |\n",
    "| A2C | ![](img/training_a2c_backtest1.jpg) | ![](img/evaluation_a2c_backtest1.jpg) |\n",
    "| PPO | ![](img/training_ppo_backtest1.jpg) | ![](img/evaluation_ppo_backtest1.jpg) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-color",
   "metadata": {},
   "source": [
    "| Method  | Training Period | Evaluation Period |\n",
    "| ------------- | ------------- | ------------- |\n",
    "| A2C | ![](img/training_a2c_backtest2.jpg) | ![](img/evaluation_a2c_backtest2.jpg) |\n",
    "| PPO | ![](img/training_ppo_backtest2.jpg) | ![](img/evaluation_ppo_backtest2.jpg) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-corner",
   "metadata": {},
   "source": [
    "## Future plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-series",
   "metadata": {},
   "source": [
    "We expect to improve the current implementation by:\n",
    "- Add more technical indicators\n",
    "- Try a few more RL models like SAC, TD3\n",
    "- Change the usage of dataset: currently statistics of different stocks on the same trading day are merged to a single data point, we plan to change the dataset usage so that statistics of a single stock on a trading day can be treated as a single data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-grain",
   "metadata": {},
   "source": [
    "# References\n",
    "<a id=\"1\">[1]</a> \n",
    "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. \n",
    "Proximal Policy Optimization Algorithms.\n",
    "arXiv:1707.06347\n",
    "https://arxiv.org/abs/1707.06347   \n",
    "<a id=\"2\">[2]</a> \n",
    "Vijay Konda , John Tsitsiklis. \n",
    "Actor-critic algorithms.\n",
    "Society for Industrial and Applied Mathematics.\n",
    "https://papers.nips.cc/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf   \n",
    "<a id=\"3\">[3]</a> \n",
    "Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu.\n",
    "Asynchronous Methods for Deep Reinforcement Learning.\n",
    "arXiv:1602.01783\n",
    "https://arxiv.org/abs/1707.06347\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}